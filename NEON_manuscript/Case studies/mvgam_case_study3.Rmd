---
title: 'mvgam case study 3: distributed lag models'
author: "Nicholas Clark (n.clark@uq.edu.au)"
output:
  html_document:
    df_print: paged
  pdf_document:
    highlight: zenburn
  word_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=40),tidy=TRUE)
```

Here we will use the `mvgam` package, which fits dynamic GAMs using MCMC sampling via the `JAGS` software (Note that `JAGS` is required; installation links are found [here](https://sourceforge.net/projects/mcmc-jags/files/)), to estimate paramaters of a Bayesian distributed lag model.  These models are used to describe simultaneously non-linear and delayed functional relationships between a covariate and a response, and are sometimes referred to as exposure-lag-response models. If we assume $\tilde{\boldsymbol{y}}_{t}$ is the conditional expectation of a discrete response variable $\boldsymbol{y}$ at time $\boldsymbol{t}$, the linear predictor for a dynamic distributed lag GAM with one lagged covariate is written as:

$$log(\tilde{\boldsymbol{y}}_{t})={\boldsymbol{B}}_0+\sum\limits_{k=1}^K{f}(\boldsymbol{b}_{k,t}\boldsymbol{x}_{k,t})+\boldsymbol{z}_{t}\,,$$
where $\boldsymbol{B}_{0}$ is the unknown intercept, the $\boldsymbol{b}$'s are unknown spline coefficients estimating how the functional effect of covariate ($\boldsymbol{x}$) on $log(\tilde{\boldsymbol{y}}_{t})$ changes over increasing lags (up to a maximum lag of ($\boldsymbol{K}$)) and $\boldsymbol{z}$ is a dynamic latent trend component. 

To demonstrate how these models are estimated in `mvgam`, first we load the Portal rodents capture data, which are available from the `portalr` package
```{r message=FALSE, warning = FALSE}
#devtools::install_github("nicholasjclark/mvgam")
library(mvgam)
library(dplyr)
portal_dat <- read.csv('https://raw.githubusercontent.com/nicholasjclark/mvgam/master/NEON_manuscript/Case studies/rodents_data.csv', as.is = T)
```

We'll keep data from the year 2004 onwards to make the model quicker to estimate for this simple example
```{r}
portal_dat %>%
  dplyr::filter(year >= 2004) %>%
  dplyr::group_by(year, month) %>%
  dplyr::slice_head(n = 1) -> portal_dat_all
```

Below is an exact reproduction of Simon Wood's lag matrix function (which he uses in his distributed lag example from his book [Generalized Additive Models - An Introduction with R 2nd edition](https://www.taylorfrancis.com/books/mono/10.1201/9781315370279/generalized-additive-models-simon-wood)). Here we supply a vector and specify the maximum lag that we want, and it will return a matrix of dimension `length(x) * lag`. Note that `NAs` are used for the missing lag values at the beginning of the matrix. In essence, the matrix objects represent exposure histories, where each row represents the lagged values of the predictor that correspond to each observation in `y`
```{r}
lagard <- function(x, n.lag = 6) {
  n <- length(x); X <- matrix(NA, n, n.lag)
  for (i in 1:n.lag) X[i:n, i] <- x[i:n - i + 1]
  X
}
```

Organise all data needed for modelling into a list. We will focus only on the species *Chaetodipus penicillatus* (labelled as `PP`), which shows reasonable seasonality in its captures over time
```{r}
data_all <- list(lag=matrix(0:5,nrow(portal_dat_all),6,byrow=TRUE),
            y = portal_dat_all$PP,
            season = portal_dat_all$month,
            year = portal_dat_all$year,
            series = rep(as.factor('series1'), NROW(portal_dat_all)),
            time = 1:NROW(portal_dat_all))
data_all$precip <- lagard(portal_dat_all$precipitation)
data_all$mintemp <- lagard(portal_dat_all$mintemp)
```

The exposure history matrix elements of the data list look as follows:
```{r}
head(data_all$lag, 5)
head(data_all$precip, 5)
head(data_all$mintemp, 5)
```

All other elements of the data list are in the usual vector format
```{r}
head(data_all$y, 5)
head(data_all$series, 5)
head(data_all$year, 5)
head(data_all$time, 5)
```

View the raw series. There is a clear seasonal pattern to the data, and there are missing values scattered throughout
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot(ts(data_all$y, frequency = 12), ylab = 'Captures for PP', xlab = '',
     lwd = 2.5, col = "#8F2727")
```

Create training and testing sets; start at observation 7 so that the `NA` values at the beginning of the covariate lag matrices are not included. Currently there is no option for on-the-fly imputation of missing covariate values in `mvgam` models, though this can easily be done in `JAGS` by specifying prior distributions over these missing entries
```{r}
data_train <- list(lag = data_all$lag[7:174,],
                   y = data_all$y[7:174],
                   series = data_all$series[7:174],
                   season = data_all$season[7:174],
                   year = data_all$year[7:174],
                   time = 7:174,
                   precip = data_all$precip[7:174,],
                   mintemp = data_all$mintemp[7:174,])
data_test <- list(lag = data_all$lag[175:length(data_all$y),],
                   y = data_all$y[175:length(data_all$y)],
                   series = data_all$series[175:length(data_all$y)],
                   season = data_all$season[175:length(data_all$y)],
                   year = data_all$year[175:length(data_all$y)],
                   time = 175:length(data_all$y),
                   precip = data_all$precip[175:length(data_all$y),],
                  mintemp = data_all$mintemp[175:length(data_all$y),])
```

Now we can fit a dynamic GAM with distributed lag terms for precipitation and minimum temperature and an `AR1` process for the unobserved temporal trend. The distributed lags are set up as tensor product smooth functions (see `help(te)` for an explanation of tensor product smooth constructions in the `mgcv` package) between `lag` and each covariate. Given the overdispersion in the data, we will assume a [`Geometric-Poisson`](https://en.wikipedia.org/wiki/Geometric_Poisson_distribution) observation model, which can be [more flexible than the `Negative binomial` for modelling overdispersed count data](https://www.jstor.org/stable/2533492?origin=crossref&seq=1). In `mvgam` the `Geometric-Poisson` is estimated as a `Tweedie-Poisson` model with the power parameter `p` fixed at `1.5`
```{r, message=FALSE, warning=FALSE}
mod1 <- mvjagam(formula =  y ~ te(mintemp, lag, k = c(8, 4)) +
                 te(precip, lag, k = c(8, 4)),
                data_train = data_train,
                data_test = data_test,
                family = 'tw',
                chains = 4,
                burnin = 20000,
                trend_model = 'AR1')
```

The summary of the model provides useful information on convergence for unobserved parameters
```{r}
summary(mod1)
```

The overdispersion parameter of the `Tweedie-Poisson` distribution is not converging, most likely because it is taking on very small values and any change in this region does not have a major impact on the resulting predictions. This is evidence that overdispersion is not fully supported. We may therefore be inclined to simplify the model and estimate a `Poisson` observation process instead. A posterior summary of this model indicates better performance of the samplers
```{r, message=FALSE, warning=FALSE}
mod2 <- mvjagam(formula =  y ~ te(mintemp, lag, k = c(8, 4)) +
                 te(precip, lag, k = c(8, 4)),
                data_train = data_train,
                data_test = data_test,
                family = 'poisson',
                chains = 4,
                burnin = 20000,
                trend_model = 'AR1')
summary(mod2)
```

To further ensure our model is capable of representing the dispersion of the observed time series, a posterior predictive rootogram can be visualised. This plot compares the frequencies of observed vs predicted values for each bin, which can help to identify aspects of poor model fit. For example, if the gray bars (representing observed frequencies) tend to stretch below zero, this suggests the model's simulations predict the values in that particular bin less frequently than they are observed in the data. A well-fitting model that can generate realistic simulated data will provide a rootogram in which the lower boundaries of the grey bars are generally near zero
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
ppc(mod2, type = 'rootogram')
```

As with all other `mvgam` objects, we can create plots of the estimated forecast distribution and the dynamic latent trend process (with 1st derivatives shown to inspect time periods of substantial upward or downward change in the latent trend)
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_fc(mod2, series = 1, data_test = data_test, ylim = c(0, 100))
```

```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_trend(mod2, series = 1, data_test = data_test, derivatives = T)
```

Traceplots of smooth penalties and latent trend parameters indicate good mixing and convergence of the four MCMC chains, as well as providing evidence that the data have informed posterior estimates that have shifted away from the prior distributions for many of these parameters
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_trace(mod2, 'rho')
plot_mvgam_trace(mod2, 'trend')
```

We can also create quick plots of the estimated smooth tensor product interactions for the distributed lag terms, which basically follow `mgcv`'s two-dimensional plotting utility but uses the `mvgam`'s estimated coefficients
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_smooth(mod2, series = 1, smooth = 1)
```

```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_smooth(mod2, series = 1, smooth = 2)
```

If you are like me then you'll find these plots rather difficult to interpret! The more intense yellow/white colours indicate higher predicted values, with the deeper red colours representing lower predicted values, but actually making sense of how the functional response is expected to change over different lags is not easy from these plots. HOwever, we can use the `predict_mvgam` function to generate much more interpretable plots. First we will focus on the effect of `mintemp` and generate a series of predictions to visualise how the estimated function changes over different lags. Set up prediction data by zeroing out all covariates apart from the covariate of interest
```{r}
newdata <- data_test
newdata$year <- rep(0, length(newdata$year))
newdata$season <- rep(0, length(newdata$season))
newdata$precip <- matrix(0, ncol = ncol(newdata$precip),
                         nrow = nrow(newdata$precip))
```

Set up `viridis` plot colours and initiate the plot window to be centred around zero. We will then keep all `mintemp` values at zero apart from the particular lag being predicted so that we can visualise how the predicted function changes over lags of `mintemp`. Predictions are generated on the link scale in this case, though you could also use the response scale. Note that we need to first generate predictions with all covariates (including the `mintemp` covariate) zeroed out to find the 'baseline' prediction so that we can shift by this baseline for generating a zero-centred plot. That way our resulting plot will roughly follow the traditional `mgcv` partial effect plots
```{r, fig.width = 6, fig.height = 5, fig.align='center'}
cols <- viridis::inferno(6)
plot(1, type = "n",
     xlab = 'Mintemp',
     ylab = 'Predicted response function',
     xlim = c(min(data_train$mintemp), max(data_train$mintemp)),
     ylim = c(-1.6, 1.6))

# Calculate predictions for when mintemp is all zeros to find the baseline
# value for centring the plot
newdata$mintemp <- matrix(0, ncol = ncol(newdata$mintemp),
                         nrow = nrow(newdata$mintemp))
preds <- predict(mod2, series = 1, newdata = newdata, type = 'link')
offset <- mean(preds)

for(i in 1:6){
  # Set up prediction matrix for mintemp with lag i as the prediction sequence; 
  # use a sequence of mintemp values across the full range of observed values in the training data
  newdata$mintemp <- matrix(0, ncol = ncol(newdata$precip),
                            nrow = nrow(newdata$precip))
  newdata$mintemp[,i] <- seq(min(data_train$mintemp),
                             max(data_train$mintemp),
                             length.out = length(newdata$year))

  # Predict on the link scale and shift by the offset so that values are roughly centred at zero
  preds <- predict(mod2, series = 1, newdata = newdata, type = 'link') - offset

  # Calculate empirical prediction quantiles
  probs = c(0.05, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.95)
  cred <- sapply(1:NCOL(preds),
                 function(n) quantile(preds[,n],
                                      probs = probs))

  # Plot expected function posterior intervals (40-60%) and medians in varying colours per lag
  pred_upper <- cred[4,]
  pred_lower <- cred[6,]
  pred_vals <- seq(min(data_train$mintemp),
                   max(data_train$mintemp),
                   length.out = length(newdata$year))
  polygon(c(pred_vals, rev(pred_vals)), c(pred_upper, rev(pred_lower)),
          col = scales::alpha(cols[i], 0.6), border = scales::alpha(cols[i], 0.7))
  lines(pred_vals, cred[5,],
        col = scales::alpha(cols[i], 0.8), lwd = 2.5)
}
abline(h = 0, lty = 'dashed')
legend('topleft', legend = paste0('lag', seq(0, 5)),
       bg = 'white', bty = 'n',
       col = cols, lty = 1, lwd = 6)
```

This plot demonstrates how the effect of `mintemp` is expected to change over different exposure lags, with the 3 - 5 month lags showing more of a cyclic seasonal pattern (catches expected to increase in the summer and autumn, roughly 3 - 5 months following cold minimum winter temperatures) while the recent lags (lags 0 and 1) demonstrate a more linear response function (catches broadly increasing as minimum temperature increases). This is hopefully a useful example for developing a better understanding of how a distributed lag model is attempting to recreate the data generating process. And here is the same plot for precipitation, which demonstrates how a u-shaped functional relationship diminishes toward a flat function at lags 2 - 5 (though this effect is clearly less important in the model than the mintemp * lag effect above)
```{r, fig.width = 6, fig.height = 5, fig.align='center'}
newdata <- data_test
newdata$year <- rep(0, length(newdata$year))
newdata$season <- rep(0, length(newdata$season))
newdata$mintemp <- matrix(0, ncol = ncol(newdata$mintemp),
                         nrow = nrow(newdata$mintemp))
newdata$precip <- matrix(0, ncol = ncol(newdata$precip),
                         nrow = nrow(newdata$precip))
preds <- predict(mod2, series = 1, newdata = newdata, type = 'link')
offset <- mean(preds)
plot(1, type = "n",
     xlab = 'Precipitation',
     ylab = 'Predicted response function',
     xlim = c(min(data_train$precip), max(data_train$precip)),
     ylim = c(-1.6, 1.6))

for(i in 1:6){
  newdata$precip <- matrix(0, ncol = ncol(newdata$precip),
                            nrow = nrow(newdata$precip))
  newdata$precip[,i] <- seq(min(data_train$precip),
                             max(data_train$precip),
                             length.out = length(newdata$year))
  preds <- predict(mod2, series = 1, newdata = newdata, type = 'link') - offset
  probs = c(0.05, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.95)
  cred <- sapply(1:NCOL(preds),
                 function(n) quantile(preds[,n],
                                      probs = probs))
  pred_upper <- cred[4,]
  pred_lower <- cred[6,]
  pred_vals <- seq(min(data_train$precip),
                   max(data_train$precip),
                   length.out = length(newdata$year))
  polygon(c(pred_vals, rev(pred_vals)), c(pred_upper, rev(pred_lower)),
          col = scales::alpha(cols[i], 0.6), border = scales::alpha(cols[i], 0.7))
  lines(pred_vals, cred[5,],
        col = scales::alpha(cols[i], 0.8), lwd = 2.5)
}
abline(h = 0, lty = 'dashed')
legend('topleft', legend = paste0('lag', seq(0, 5)),
       bg = 'white', bty = 'n',
       col = cols, lty = 1, lwd = 6)
```

All of the usual functions in `mvgam` can also be used for list data objects, so long as they contain the necessary fields `series`, `season` and `year`. For example, posterior retrodictive checks for the in-sample training period: 
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
ppc(mod2, series = 1, type = 'cdf')
```

and predictive checks for the out of sample forecast period (which demonstrates how the model tends to overpredict for the forecast period in this particular example):
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
ppc(mod2, data_test = data_test, series = 1, type = 'cdf')
```

Logical next steps for interrogating this model would be to trial different trend types (i.e. random walk), replace the distributed lag function for `precip` with a standard smooth function (that does not include lag interactions, as clearly the model above indicates that these are not supported) and inspect whether different covariates (such as `ndvi` or `maxtemp`) might play a role in modulating catches of `PP`. Finally, once we are satisfied that we have a well-performing model that we can understand and interrogate, we could expand up to a multivariate model by including other species as response variables. This would allow us to capture any possible unobserved dependencies in the catches of multiple co-occurring species in a single unified modelling framework
