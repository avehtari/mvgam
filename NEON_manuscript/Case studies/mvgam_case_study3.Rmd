---
title: 'mvgam case study 3: distributed lag models'
author: "Nicholas Clark (n.clark@uq.edu.au)"
output:
  html_document:
    df_print: paged
  pdf_document:
    highlight: zenburn
  word_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=40),tidy=TRUE)
```

Here we will use the `mvgam` package, which fits dynamic GAMs using MCMC sampling via either the `JAGS` software (installation links are found [here](https://sourceforge.net/projects/mcmc-jags/files/)) or via the `Stan` software (installation links are found [here](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started)), to estimate paramaters of a Bayesian distributed lag model.  These models are used to describe simultaneously non-linear and delayed functional relationships between a covariate and a response, and are sometimes referred to as exposure-lag-response models. If we assume $\tilde{\boldsymbol{y}}_{t}$ is the conditional expectation of a discrete response variable $\boldsymbol{y}$ at time $\boldsymbol{t}$, the linear predictor for a dynamic distributed lag GAM with one lagged covariate is written as:

$$log(\tilde{\boldsymbol{y}}_{t})={\boldsymbol{B}}_0+\sum\limits_{k=1}^K{f}(\boldsymbol{b}_{k,t}\boldsymbol{x}_{k,t})+\boldsymbol{z}_{t}\,,$$
where $\boldsymbol{B}_{0}$ is the unknown intercept, the $\boldsymbol{b}$'s are unknown spline coefficients estimating how the functional effect of covariate ($\boldsymbol{x}$) on $log(\tilde{\boldsymbol{y}}_{t})$ changes over increasing lags (up to a maximum lag of ($\boldsymbol{K}$)) and $\boldsymbol{z}$ is a dynamic latent trend component. 

To demonstrate how these models are estimated in `mvgam`, first we load the Portal rodents capture data, which are available from the `portalr` package
```{r message=FALSE, warning = FALSE}
#devtools::install_github("nicholasjclark/mvgam")
library(mvgam)
library(dplyr)
portal_dat <- read.csv('https://raw.githubusercontent.com/nicholasjclark/mvgam/master/NEON_manuscript/Case studies/rodents_data.csv', as.is = T)
```

We'll keep data from the year 2004 onwards to make the model quicker to estimate for this simple example
```{r}
portal_dat %>%
  dplyr::filter(year >= 2004) %>%
  dplyr::group_by(year, month) %>%
  dplyr::slice_head(n = 1) -> portal_dat_all
```

Below is an exact reproduction of Simon Wood's lag matrix function (which he uses in his distributed lag example from his book [Generalized Additive Models - An Introduction with R 2nd edition](https://www.taylorfrancis.com/books/mono/10.1201/9781315370279/generalized-additive-models-simon-wood)). Here we supply a vector and specify the maximum lag that we want, and it will return a matrix of dimension `length(x) * lag`. Note that `NAs` are used for the missing lag values at the beginning of the matrix. In essence, the matrix objects represent exposure histories, where each row represents the lagged values of the predictor that correspond to each observation in `y`
```{r}
lagard <- function(x, n.lag = 6) {
  n <- length(x); X <- matrix(NA, n, n.lag)
  for (i in 1:n.lag) X[i:n, i] <- x[i:n - i + 1]
  X
}
```

Organise all data needed for modelling into a list. We will focus only on the species *Chaetodipus penicillatus* (labelled as `PP`), which shows reasonable seasonality in its captures over time
```{r}
data_all <- list(lag=matrix(0:5,nrow(portal_dat_all),6,byrow=TRUE),
            y = portal_dat_all$PP,
            season = portal_dat_all$month,
            year = portal_dat_all$year,
            series = rep(as.factor('series1'), NROW(portal_dat_all)),
            time = 1:NROW(portal_dat_all))
data_all$precip <- lagard(portal_dat_all$precipitation)
data_all$mintemp <- lagard(portal_dat_all$mintemp)
```

The exposure history matrix elements of the data list look as follows:
```{r}
head(data_all$lag, 5)
head(data_all$precip, 5)
head(data_all$mintemp, 5)
```

All other elements of the data list are in the usual vector format
```{r}
head(data_all$y, 5)
head(data_all$series, 5)
head(data_all$year, 5)
head(data_all$time, 5)
```

View the raw series. There is a clear seasonal pattern to the data, and there are missing values scattered throughout
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot(ts(data_all$y, frequency = 12), ylab = 'Captures for PP', xlab = '',
     lwd = 2.5, col = "#8F2727")
```

Create training and testing sets; start at observation 7 so that the `NA` values at the beginning of the covariate lag matrices are not included. Currently there is no option for on-the-fly imputation of missing covariate values in `mvgam` models, though this can easily be done in `JAGS` by specifying prior distributions over these missing entries
```{r}
data_train <- list(lag = data_all$lag[7:174,],
                   y = data_all$y[7:174],
                   series = data_all$series[7:174],
                   season = data_all$season[7:174],
                   year = data_all$year[7:174],
                   time = 7:174,
                   precip = data_all$precip[7:174,],
                   mintemp = data_all$mintemp[7:174,])
data_test <- list(lag = data_all$lag[175:length(data_all$y),],
                   y = data_all$y[175:length(data_all$y)],
                   series = data_all$series[175:length(data_all$y)],
                   season = data_all$season[175:length(data_all$y)],
                   year = data_all$year[175:length(data_all$y)],
                   time = 175:length(data_all$y),
                   precip = data_all$precip[175:length(data_all$y),],
                  mintemp = data_all$mintemp[175:length(data_all$y),])
```

Now we can fit a Bayesian GAM with distributed lag terms for precipitation and minimum temperature. The distributed lags are set up as tensor product smooth functions (see `help(te)` for an explanation of tensor product smooth constructions in the `mgcv` package) between `lag` and each covariate. We will start simply by assuming our data follow a `Poisson` observation process
```{r, message=FALSE, warning=FALSE}
mod1 <- mvgam(formula =  y ~ te(mintemp, lag, k = c(8, 4)) +
                 te(precip, lag, k = c(8, 4)),
                data_train = data_train,
                data_test = data_test,
                family = 'poisson',
                chains = 4,
                burnin = 10000,
                trend_model = 'None')
```

Posterior predictive rootograms are a useful way to explore whether a discrete model is able to capture relevant dispersion in the observed data. This plot compares the frequencies of observed vs predicted values for each bin, which can help to identify aspects of poor model fit. For example, if the gray bars (representing observed frequencies) tend to stretch below zero, this suggests the model's simulations predict the values in that particular bin less frequently than they are observed in the data. A well-fitting model that can generate realistic simulated data will provide a rootogram in which the lower boundaries of the grey bars are generally near zero
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
ppc(mod1, type = 'rootogram')
```

The `Poisson` model is not doing a great job of capturing dispersion, underpredicting the zeros in the data and overpredicting some of the medium-range values (counts of `~5-30`). The residual `Q-Q` plot confirms that the `Poisson` is not an appropriate distribution for these data
```{r, fig.width = 6, fig.height = 5, fig.align='center'}
plot(mod1, type = 'residuals')
```


Given the overdispersion present in the data, we will now assume a [`Geometric-Poisson`](https://en.wikipedia.org/wiki/Geometric_Poisson_distribution) observation model, which can be [more flexible than the `Negative binomial` for modelling overdispersed count data](https://www.jstor.org/stable/2533492?origin=crossref&seq=1). In `mvgam` the `Geometric-Poisson` is estimated as a `Tweedie-Poisson` model with the power parameter `p` fixed at `1.5`
```{r, message=FALSE, warning=FALSE}
mod2 <- mvgam(formula =  y ~ te(mintemp, lag, k = c(8, 4)) +
                 te(precip, lag, k = c(8, 4)),
                data_train = data_train,
                data_test = data_test,
                family = 'tw',
                chains = 4,
                burnin = 10000,
                trend_model = 'None')
```

The rootogram for this model looks better, though of course there is still some overprediction of medium-range values
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
ppc(mod2, type = 'rootogram')
```

However, the residual plot looks much better for this model
```{r, fig.width = 6, fig.height = 5, fig.align='center'}
plot(mod2, type = 'residuals')
```

The summary of the model provides useful information on convergence for unobserved parameters. Notice how strongly positive the overdispersion parameter is estimated to be, providing further evidence that this overdispersion is important to capture for these data
```{r}
summary(mod2)
```

As this is a timeseries and the residual plot hints at some autocorrelation remaining in the short-term lags, lets check if an `AR` latent trend process improves forecasts compared to the no-trend model. An important note here is the choice of prior for the overdispersion parameter `twdis`. This parameter and the latent trend variance can interact strongly, particularly when overdispersion is in the data high. This is because at high values of the dispersion parameter, there is less need for a latent trend to be able to capture any outliers and so the latent trend precision can go up toward infinity, approaching a space of very diffuse likelihood that forces the Gibbs samplers to take on frustratingly small step sizes. Likewise when there is not much need for overdispersion, the dispersion parameter can approach zero and move around in an equally uninformative parameter space. The latent trend operates on the log scale, so really we should not expect autocorrelated jumps in trappings of more than `6-8` from timepoint to timepoint (any larger and the trend will compete strongly with the overdispersion parameter, making it difficult for us to model the inherent overdispersion process and instead assuming it is all autocorrelation). A containment prior on the latent trend `sigma` will help achieve this
```{r, message=FALSE, warning=FALSE}
mod3 <- mvgam(formula =  y ~ te(mintemp, lag, k = c(8, 4)) +
                 te(precip, lag, k = c(8, 4)),
                data_train = data_train,
                data_test = data_test,
                family = 'tw',
                sigma_prior = 'dexp(2.5)T(0.15, 2)',
                chains = 4,
                burnin = 10000,
                trend_model = 'AR3')
summary(mod3)
```

A pairs plot of the logged versions of the latent trend precision and the overdispersion parameter suggest there is no strange behaviour in the joint posterior
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot(log(MCMCvis::MCMCchains(mod3$model_output, 'twdis')),
     log(MCMCvis::MCMCchains(mod3$model_output, 'tau')),
     ylab = 'log(tau)', xlab = 'log(twdis)', pch = 16, col = "#8F272740")
```

Our rootogram has not improved much with the addition of the latent trend
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
ppc(mod3, type = 'rootogram')
```

But there is no more evidence of autocorrelation in the residuals
```{r, fig.width = 6, fig.height = 5, fig.align='center'}
plot(mod3, type = 'residuals')
```
We can also demonstrate another feature of `mvgam`, which is the ability to use Hamiltonian Monte Carlo for parameter estimation via the software `Stan` (using the `rstan` interface). Note that `rstan` is currently required for this option to work, though support for other `Stan` interfaces will be added in future. Also note that currently there is no support for fitting `Tweedie` responses or dynamic factor models in `Stan`, though again these will be added in future. Because of these current limitations, we will stick with a `Negative Binomial` observation process for the `Stan` version. However there are great advantages when using `Stan`, which includes the option to estimate smooth latent trends via [Hilbert space approximate Gaussian Processes](https://arxiv.org/abs/2004.11408). This often makes sense for ecological series, which we expect to change smoothly over time. As expected when compared to the Gibbs sampler in `JAGS`, the `Stan` version converges very nicely
```{r, message=FALSE, warning=FALSE}
mod4 <- mvgam(formula =  y ~ te(mintemp, lag, k = c(8, 4)) +
                 te(precip, lag, k = c(8, 4)),
                data_train = data_train,
                data_test = data_test,
                family = 'nb',
                chains = 4,
                burnin = 2000,
                trend_model = 'GP',
              use_stan = T)
summary(mod4)
```

As with all other `mvgam` objects, we can create plots of the estimated forecast distribution
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_fc(mod4, series = 1, data_test = data_test, ylim = c(0, 100))
```
The trend now evolves smoothly via an infinite dimensional Gaussian Process
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_trend(mod4, series = 1)
```

Traceplots of smooth penalties indicate good mixing and convergence of the four MCMC chains
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_trace(mod4, 'rho')
```

We can also create quick plots of the estimated smooth tensor product interactions for the distributed lag terms, which basically follow `mgcv`'s two-dimensional plotting utility but uses the `mvgam`'s estimated coefficients
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_smooth(mod4, series = 1, smooth = 1)
```

```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_smooth(mod4, series = 1, smooth = 2)
```

If you are like me then you'll find these plots rather difficult to interpret! The more intense yellow/white colours indicate higher predicted values, with the deeper red colours representing lower predicted values, but actually making sense of how the functional response is expected to change over different lags is not easy from these plots. HOwever, we can use the `predict_mvgam` function to generate much more interpretable plots. First we will focus on the effect of `mintemp` and generate a series of predictions to visualise how the estimated function changes over different lags. Set up prediction data by zeroing out all covariates apart from the covariate of interest
```{r}
newdata <- data_test
newdata$year <- rep(0, length(newdata$year))
newdata$season <- rep(0, length(newdata$season))
newdata$precip <- matrix(0, ncol = ncol(newdata$precip),
                         nrow = nrow(newdata$precip))
```

Set up `viridis` plot colours and initiate the plot window to be centred around zero. We will then keep all `mintemp` values at zero apart from the particular lag being predicted so that we can visualise how the predicted function changes over lags of `mintemp`. Predictions are generated on the link scale in this case, though you could also use the response scale. Note that we need to first generate predictions with all covariates (including the `mintemp` covariate) zeroed out to find the 'baseline' prediction so that we can shift by this baseline for generating a zero-centred plot. That way our resulting plot will roughly follow the traditional `mgcv` partial effect plots
```{r, fig.width = 6, fig.height = 5, fig.align='center'}
cols <- viridis::inferno(6)
plot(1, type = "n",
     xlab = 'Mintemp',
     ylab = 'Predicted response function',
     xlim = c(min(data_train$mintemp), max(data_train$mintemp)),
     ylim = c(-1.6, 1.6))

# Calculate predictions for when mintemp is all zeros to find the baseline
# value for centring the plot
newdata$mintemp <- matrix(0, ncol = ncol(newdata$mintemp),
                         nrow = nrow(newdata$mintemp))
preds <- predict(mod4, series = 1, newdata = newdata, type = 'link')
offset <- mean(preds)

for(i in 1:6){
  # Set up prediction matrix for mintemp with lag i as the prediction sequence; 
  # use a sequence of mintemp values across the full range of observed values in the training data
  newdata$mintemp <- matrix(0, ncol = ncol(newdata$precip),
                            nrow = nrow(newdata$precip))
  newdata$mintemp[,i] <- seq(min(data_train$mintemp),
                             max(data_train$mintemp),
                             length.out = length(newdata$year))

  # Predict on the link scale and shift by the offset so that values are roughly centred at zero
  preds <- predict(mod4, series = 1, newdata = newdata, type = 'link') - offset

  # Calculate empirical prediction quantiles
  probs = c(0.05, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.95)
  cred <- sapply(1:NCOL(preds),
                 function(n) quantile(preds[,n],
                                      probs = probs))

  # Plot expected function posterior intervals (40-60%) and medians in varying colours per lag
  pred_upper <- cred[4,]
  pred_lower <- cred[6,]
  pred_vals <- seq(min(data_train$mintemp),
                   max(data_train$mintemp),
                   length.out = length(newdata$year))
  polygon(c(pred_vals, rev(pred_vals)), c(pred_upper, rev(pred_lower)),
          col = scales::alpha(cols[i], 0.6), border = scales::alpha(cols[i], 0.7))
  lines(pred_vals, cred[5,],
        col = scales::alpha(cols[i], 0.8), lwd = 2.5)
}
abline(h = 0, lty = 'dashed')
legend('topleft', legend = paste0('lag', seq(0, 5)),
       bg = 'white', bty = 'n',
       col = cols, lty = 1, lwd = 6)
```

This plot demonstrates how the effect of `mintemp` is expected to change over different exposure lags, with the 3 - 5 month lags showing more of a cyclic seasonal pattern (catches expected to increase in the summer and autumn, roughly 3 - 5 months following cold minimum winter temperatures) while the recent lags (lags 0 and 1) demonstrate a more linear response function (catches broadly increasing as minimum temperature increases). This is hopefully a useful example for developing a better understanding of how a distributed lag model is attempting to recreate the data generating process. And here is the same plot for precipitation, which demonstrates how a u-shaped functional relationship diminishes toward a flat function at lags 2 - 5 (though this effect is clearly less important in the model than the mintemp * lag effect above)
```{r, fig.width = 6, fig.height = 5, fig.align='center'}
newdata <- data_test
newdata$year <- rep(0, length(newdata$year))
newdata$season <- rep(0, length(newdata$season))
newdata$mintemp <- matrix(0, ncol = ncol(newdata$mintemp),
                         nrow = nrow(newdata$mintemp))
newdata$precip <- matrix(0, ncol = ncol(newdata$precip),
                         nrow = nrow(newdata$precip))
preds <- predict(mod4, series = 1, newdata = newdata, type = 'link')
offset <- mean(preds)
plot(1, type = "n",
     xlab = 'Precipitation',
     ylab = 'Predicted response function',
     xlim = c(min(data_train$precip), max(data_train$precip)),
     ylim = c(-1.6, 1.6))

for(i in 1:6){
  newdata$precip <- matrix(0, ncol = ncol(newdata$precip),
                            nrow = nrow(newdata$precip))
  newdata$precip[,i] <- seq(min(data_train$precip),
                             max(data_train$precip),
                             length.out = length(newdata$year))
  preds <- predict(mod4, series = 1, newdata = newdata, type = 'link') - offset
  probs = c(0.05, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.95)
  cred <- sapply(1:NCOL(preds),
                 function(n) quantile(preds[,n],
                                      probs = probs))
  pred_upper <- cred[4,]
  pred_lower <- cred[6,]
  pred_vals <- seq(min(data_train$precip),
                   max(data_train$precip),
                   length.out = length(newdata$year))
  polygon(c(pred_vals, rev(pred_vals)), c(pred_upper, rev(pred_lower)),
          col = scales::alpha(cols[i], 0.6), border = scales::alpha(cols[i], 0.7))
  lines(pred_vals, cred[5,],
        col = scales::alpha(cols[i], 0.8), lwd = 2.5)
}
abline(h = 0, lty = 'dashed')
legend('topleft', legend = paste0('lag', seq(0, 5)),
       bg = 'white', bty = 'n',
       col = cols, lty = 1, lwd = 6)
```

All of the usual functions in `mvgam` can also be used for list data objects and for models fitted with `Stan`, so long as they contain the necessary fields `series`, `season` and `year`. For example, posterior retrodictive checks for the in-sample training period: 
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
ppc(mod4, series = 1, type = 'cdf')
```

and predictive checks for the out of sample forecast period (which demonstrates how the model tends to overpredict for the forecast period in this particular example):
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
ppc(mod4, data_test = data_test, series = 1, type = 'cdf')
```

Logical next steps for interrogating this model would be to trial different trend types (i.e. random walk), replace the distributed lag function for `precip` with a standard smooth function (that does not include lag interactions, as clearly the model above indicates that these are not supported) and inspect whether different covariates (such as `ndvi` or `maxtemp`) might play a role in modulating catches of `PP`. Finally, once we are satisfied that we have a well-performing model that we can understand and interrogate, we could expand up to a multivariate model by including other species as response variables. This would allow us to capture any possible unobserved dependencies in the catches of multiple co-occurring species in a single unified modelling framework
