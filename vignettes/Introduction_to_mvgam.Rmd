---
title: "Introduction to mvgam"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction_to_mvgam}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(rmarkdown.html_vignette.check_title = FALSE)
```

In this vignette we will introduce dynamic Generalised Additive Models and some of the key utility functions provided in `mvgam`. First a univariate example to show how challenging it can be to forecast ahead with conventional GAMs and how `mvgam` overcomes these challenges. We begin by replicating the lynx analysis from [2018 Ecological Society of America workshop on GAMs](https://noamross.github.io/mgcv-esa-2018/) that was hosted by Eric Pedersen, David L. Miller, Gavin Simpson, and Noam Ross, with some minor adjustments. First, load the data and plot the series as well as its estimated autocorrelation function
```{r, fig.show='hold', fig.align='center'}
library(mvgam)
data(lynx)
lynx_full = data.frame(year = 1821:1934, 
                       population = as.numeric(lynx))
plot(lynx_full$population, type = 'l', ylab = 'Lynx trappings',
     xlab = 'Time')
acf(lynx_full$population, main = '')
```

There is a clear ~19-year cyclic pattern to the data, so I create a `season` term that can be used to model this effect and give a better representation of the data generating process. I also put the year term on a more manageable scale
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot(stl(ts(lynx_full$population, frequency = 19), s.window = 'periodic'))
lynx_full$season <- (lynx_full$year %%19) + 1
lynx_full$year <- lynx_full$year - 1820
```

Add lag indicators needed to fit the nonlinear lag models that gave the best one step ahead point forecasts in the ESA workshop example. As in the example, we specify the `default` argument in the `lag` function as the mean log population.
```{r}
mean_pop_l = mean(log(lynx_full$population))
lynx_full = dplyr::mutate(lynx_full,
                   popl = log(population),
                   lag1 = dplyr::lag(popl,1, default = mean_pop_l),
                   lag2 = dplyr::lag(popl,2, default = mean_pop_l),
                   lag3 = dplyr::lag(popl,3, default = mean_pop_l),
                   lag4 = dplyr::lag(popl,4, default = mean_pop_l),
                   lag5 = dplyr::lag(popl,5, default = mean_pop_l),
                   lag6 = dplyr::lag(popl,6, default = mean_pop_l))
```

For `mvgam` models, the response needs to be labelled `y` and we also need an indicator of the series name as a `factor` variable
```{r}
lynx_full$y <- lynx_full$population
lynx_full$series <- factor('series1')
```

Split the data into training (first 74 years) and testing (next 10 years of data) to evaluate multi-step ahead forecasts
```{r}
lynx_train = lynx_full[1:75, ]
lynx_test = lynx_full[76:85, ]
```

The best-forecasting model in the course was with nonlinear smooths of lags 1 and 2; we use those here is that we also include a cyclic smooth for the 19-year cycles as this seems like an important feature, as well as a yearly smooth for the long-term trend with penalty on the first derivative to prevent linear extrapolation into the future
```{r}
lynx_mgcv = gam(population ~ s(season, bs = 'cc', k = 10) +
                 s(year, k = 5, m = 1) + s(lag1, k = 5) +
                  s(lag2, k = 5),
               knots = list(season = c(0.5, 19.5)),
                data = lynx_train, family = "poisson",
                method = "REML")
```

Inspect the model's summary and estimated smooth functions for the season, year and lag terms
```{r}
summary(lynx_mgcv)
plot(lynx_mgcv, select = 1)
plot(lynx_mgcv, select = 2)
plot(lynx_mgcv, select = 3)
plot(lynx_mgcv, select = 4)
```


This model captures most of the deviance in the series and the functions are all confidently estimated to be non-zero and non-flat. So far, so good. Now for some forecasts for the out of sample period. First we must take posterior draws of smooth beta coefficients to incorporate the uncertainties around smooth functions when simulating forecast paths
```{r}
coef_sim <- gam.mh(lynx_mgcv)$bs
```

Now we define a function to perform forecast simulations from the nonlinear lag model in a recursive fashion. Using starting values for the last two lags, the function will iteratively project the path ahead with a random sample from the model's coefficient posterior
```{r}
recurse_nonlin = function(model, lagged_vals, h){
  # Initiate state vector
  states <- rep(NA, length = h + 2)
  # Last two values of the conditional expectations begin the state vector
  states[1] <- as.numeric(exp(lagged_vals[2]))
  states[2] <- as.numeric(exp(lagged_vals[1]))
  # Get a random sample of the smooth coefficient uncertainty matrix
  # to use for the entire forecast horizon of this particular path
  gam_coef_index <- sample(seq(1, NROW(coef_sim)), 1, T)
  # For each following timestep, recursively predict based on the
  # predictions at each previous lag
  for (t in 3:(h + 2)) {
    # Build the GAM linear predictor matrix using the two previous lags
    # of the (log) density
    newdata <- data.frame(lag1 = log(states[t-1] + 0.01),
                          lag2 = log(states[t-2] + 0.01),
                          season = lynx_test$season[t-2],
                          year = lynx_test$year[t-2])
    colnames(newdata) <- c('lag1', 'lag2', 'season', 'year')
    Xp <- predict(model, newdata = newdata, type = 'lpmatrix')
    # Calculate the posterior prediction for this timepoint
    mu <- rpois(1, lambda = exp(Xp %*% coef_sim[gam_coef_index,]))
    # Fill in the state vector and iterate to the next timepoint
    states[t] <- mu
  }
  # Return the forecast path
  states[-c(1:2)]
}
```

Create the GAM's forecast distribution by generating `1000` simulated forecast paths. Each path is fed the true observed values for the last two lags of the first out of sample timepoint, but they can deviate when simulating ahead depending on their particular draw of possible coefficients. Note, this is a bit slow and could easily be parallelised to speed up computations
```{r}
gam_sims <- matrix(NA, nrow = 1000, ncol = 10)
for(i in 1:1000){
  gam_sims[i,] <- recurse_nonlin(lynx_mgcv,
                                 lagged_vals = c(lynx_test$lag1[1],
                                                 lynx_test$lag2[1]),
                                 h = 10)
}
```

Plot the mgcv model's out of sample forecast for the next 8 years ahead
```{r, fig.width=5, fig.height=4, fig.align='center'}
cred_ints <- apply(gam_sims, 2, function(x) hpd(x, 0.95))
yupper <- max(lynx_full$population) * 1.25
plot(cred_ints[3,] ~ seq(1:NCOL(cred_ints)), type = 'l',
     col = rgb(1,0,0, alpha = 0),
     ylim = c(0, yupper),
     ylab = 'Predicted lynx trappings',
     xlab = 'Forecast horizon',
     main = 'mgcv')
polygon(c(seq(1:(NCOL(cred_ints))), rev(seq(1:NCOL(cred_ints)))),
        c(cred_ints[1,],rev(cred_ints[3,])),
        col = rgb(150, 0, 0, max = 255, alpha = 100), border = NA)
cred_ints <- apply(gam_sims, 2, function(x) hpd(x, 0.68))
polygon(c(seq(1:(NCOL(cred_ints))), rev(seq(1:NCOL(cred_ints)))),
        c(cred_ints[1,],rev(cred_ints[3,])),
        col = rgb(150, 0, 0, max = 255, alpha = 180), border = NA)
lines(cred_ints[2,], col = rgb(150, 0, 0, max = 255), lwd = 2, lty = 'dashed')
points(lynx_test$population[1:10], pch = 16)
lines(lynx_test$population[1:10])
```

A decent forecast? The shape is certainly correct, but the 95% uncertainty intervals are far too narrow due to the interpolation of the precisely estimated lag smooth functions and the flat extrapolation of the yearly smooth. Now fit an `mvgam` model; it fits a similar model to the above but with a full time series model for the errors, rather than smoothing splines that do not incorporate a concept of the future. We also exclude the `year` term to reduce any possible extrapolation and because latent trend model will capture this temporal variation. We estimate the model in `JAGS` using MCMC sampling (Note that `JAGS` 4.3.0 is required; installation links are found [here](https://sourceforge.net/projects/mcmc-jags/files/))
```{r}
lynx_mvgam <- mvjagam(data_train = lynx_train,
               data_test = lynx_test,
               formula = y ~ s(season, bs = 'cc', k = 10),
               knots = list(season = c(0.5, 19.5)),
               family = 'poisson',
               trend_model = 'AR1',
               n.burnin = 60000,
               n.iter = 10000,
               thin = 10,
               auto_update = F)
```

Calculate the out of sample forecast from the fitted `mvgam` model
```{r, fig.width=5, fig.height=4, fig.align='center'}
fits <- MCMCvis::MCMCchains(lynx_mvgam$jags_output, 'ypred')
fits <- fits[,(NROW(lynx_mvgam$obs_data)+1):(NROW(lynx_mvgam$obs_data)+10)]
cred_ints <- apply(fits, 2, function(x) hpd(x, 0.95))
plot(cred_ints[3,] ~ seq(1:NCOL(cred_ints)), type = 'l',
     col = rgb(1,0,0, alpha = 0),
     ylim = c(0, yupper),
     ylab = '',
     xlab = 'Forecast horizon',
     main = 'mvgam')
polygon(c(seq(1:(NCOL(cred_ints))), rev(seq(1:NCOL(cred_ints)))),
        c(cred_ints[1,],rev(cred_ints[3,])),
        col = rgb(150, 0, 0, max = 255, alpha = 100), border = NA)
cred_ints <- apply(fits, 2, function(x) hpd(x, 0.68))
polygon(c(seq(1:(NCOL(cred_ints))), rev(seq(1:NCOL(cred_ints)))),
        c(cred_ints[1,],rev(cred_ints[3,])),
        col = rgb(150, 0, 0, max = 255, alpha = 180), border = NA)
lines(cred_ints[2,], col = rgb(150, 0, 0, max = 255), lwd = 2, lty = 'dashed')
points(lynx_test$population[1:10], pch = 16)
lines(lynx_test$population[1:10])
```

The `mvgam` has much more realistic uncertainty than the `mgcv` version, with all out of sample observations falling within the model's 95% credible intervals. Of course this is just one out of sample test of the model, and to really determine which model is most appropriate for forecasting we would want to run many of these tests using a [rolling window approach](https://robjhyndman.com/hyndsight/tscv/). Have a look at this model's summary to see what is being estimated (note that longer MCMC runs would probably be needed to increase effective sample sizes)
```{r}
summary_mvgam(lynx_mvgam)
```

Now inspect each model's estimated smooth for the 19-year cyclic pattern. Note that the `mvgam` smooth plot is on a different scale compared to the `mgcv` plot, but interpretation is similar
```{r, fig.show='hold', fig.align='center'}
plot(lynx_mgcv, select=1, shade=T)
plot_mvgam_smooth(lynx_mvgam, 1, 'season')
```

We can also view the mvgam's posterior predictions for the entire series (testing and training)
```{r, fig.width=5, fig.height=4, fig.align='center'}
plot_mvgam_fc(lynx_mvgam, data_test = lynx_test)
```

And the estimated trend
```{r, fig.width=5, fig.height=4, fig.align='center'}
plot_mvgam_trend(lynx_mvgam, data_test = lynx_test)
```

A key aspect of ecological forecasting is to understand [how different components of a model contribute to forecast uncertainty](https://esajournals.onlinelibrary.wiley.com/doi/full/10.1002/eap.1589). We can estimate contributions to forecast uncertainty for the GAM smooth functions and the latent trend using `mvgam`
```{r, fig.width=5, fig.height=4, fig.align='center'}
plot_mvgam_uncertainty(lynx_mvgam, data_test = lynx_test)
```

Clearly the trend dominates the forecast uncertainty, which is not unexpected as this is the stochastic component of the model. There may be some long-term cyclicity in the trend based on the trend plot above, so perhaps another cyclic smooth term could be useful in the GAM linear predictor. But we will leave the model as-is for this example. Diagnostics of the model can also be performed using `mvgam`. Have a look at the model's residuals, which are posterior medians of Dunn-Smyth randomised quantile residuals so should follow approximate normality. We are primarily looking for a lack of autocorrelation, which would suggest our AR2 model is appropriate for the latent trend
```{r, fig.width=6, fig.height=6, fig.align='center'}
plot_mvgam_resids(lynx_mvgam)
```

Another useful utility of `mvgam` is the ability to use rolling window forecasts to evaluate competing models that may represent different hypotheses about the series dynamics. Here we will fit a poorly specified model to showcase how this evaluation works
```{r}
lynx_mvgam_poor <- mvjagam(data_train = lynx_train,
               data_test = lynx_test,
               formula = y ~ s(season, bs = 'gp', k = 3),
               family = 'poisson',
               trend_model = 'RW',
               n.burnin = 10000,
               n.iter = 5000,
               thin = 5,
               auto_update = F)

```

We choose a set of timepoints within the training data to forecast from, allowing us to simulate a situation where the model's parameters had already been estimated but we have only observed data up to the evaluation timepoint and would like to generate forecasts from the latent trends. Here we use year 10 as our last observation and forecast ahead for the next 10 years.
```{r}
mod1_eval <- eval_mvgam(lynx_mvgam, eval_timepoint = 10, fc_horizon = 10)
mod2_eval <- eval_mvgam(lynx_mvgam_poor, eval_timepoint = 10, fc_horizon = 10)
```

Summary statistics of the two models' out of sample Discrete Rank Probability Score (DRPS) indicate that the well-specified model performs markedly better (far lower DRPS)
```{r}
summary(mod1_eval$series1$drps)
summary(mod2_eval$series1$drps)
```

Nominal coverages for both models' 90% prediction intervals
```{r}
mean(mod1_eval$series1$in_interval)
mean(mod2_eval$series1$in_interval)
```

The `compare_mvgams` function automates this process by rolling along a set of timepoints for each model, ensuring a more in-depth evaluation of each competing model at the same set of timepoints
