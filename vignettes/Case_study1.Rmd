---
title: "Case study 1: model comparison and data assimilation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Case_study1}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(rmarkdown.html_vignette.check_title = FALSE)
```

In this vignette we will examine some of the other useful functions provided in `mvgam`, including methods to compare models based on rolling window forecasts and tools to assimilate new observations via a recursive particle filter. First load the `AirPassengers` data from the `forecast` package and convert to an `xts` object (which is easier to work with than a `ts` object)
```{r}
library(mvgam)
library(dplyr)
library(xts)
library(forecast)
data("AirPassengers")
series <- xts::as.xts(floor(AirPassengers))
colnames(series) <- c('Air')
```

View the series. Also look at the distribution of observations and an STL decomposition. There is a clear seasonal pattern as well as an increasing trend over time, and the distribution shows evidence of a skew suggestive of overdispersion
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot(series)
plot(stl(series, s.window = 'periodic'))
hist(series)
```

Next use the `series_to_mvgam` function, which converts `ts` or `xts` objects into the correct format for `mvgam`. Here we set `train_prop = 0.75`, meaning we will include ~ 75% of the observations in the training set and use the remaining ~25% for forecast validation
```{r}
fake_data <- series_to_mvgam(series, freq = 12, train_prop = 0.75)
```

Examine the returned object
```{r}
head(fake_data$data_train)
head(fake_data$data_test)
```

Fit a well-specified model in which the GAM component of the linear predictor captures the repeated seasonality (with a cyclic smooth) and the dynamic latent trend captures the residual process using AR parameters (up to order 3). We use the Negative Binomial family and a burnin length of `5000` iterations. Note that feeding the `data_test` object does not mean that these values are used in training of the model. Rather, they are included as `NA` so that we can automatically create a forecast from the posterior predictions for these observations. This is useful for plotting forecasts without needing to run new data through the model's equations later on
```{r}
mod <- mvjagam(data_train = fake_data$data_train,
               data_test = fake_data$data_test,
               formula = y ~ s(season, bs = c('cc'), k = 12),
               knots = list(season = c(0.5, 12.5)),
               family = 'nb',
               trend_model = 'AR3',
               n.burnin = 15000,
               auto_update = F)
```
               
Check the model summary to examine convergence for key parameters
```{r}
summary_mvgam(mod)
```

Plot diagnostics of posterior median Dunn-Smyth residuals to check for remaining autocorrelation and appropriateness of the Negative Binomial distribution
```{r, fig.width = 7, fig.height = 7, fig.align='center'}
plot_mvgam_resids(mod, 1)
```

Plot the seasonal smooth term and its associated credible intervals
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_smooth(mod, 1, 'season')
```

Compare the model's forecast distribution to one from an automatic exponential smoothing model (ETS) generated by the `forecast` package. This model is a useful benchmark for this series as the values are large enough that a Gaussian distribution is less inappropriate here. But note that for series with smaller counts, including zeros, as well as missing data and overdispersion, ETS models will not work. In fact, this lack of ability to fit some of the most popular automatic forecasting models to ecological time series was one of the primary motivations behind the design of the `mvgam` package
```{r, fig.width = 7.5, fig.height = 4, fig.align='center'}
par(mfrow = c(1,2))
ets_fc <- forecast(ets(series), h = NROW(fake_data$data_test))
plot_mvgam_fc(mod, ylim = c(min(ets_fc$x),
                            max(ets_fc$upper)),
              data_test = fake_data$data_test)
plot(ets_fc)
```

The trends differ here because the ETS model uses a dampening parameter to ensure the trend flattens out into the future, which is a feature that has proven to improve forecasts over longer horizons. `mvgam` does not directly implement this feature (the AR terms will indirectly control the dampening of the trend), but this shouldn't be as much of an issue for near-term and medium-term predictions. Plot the estimated latent trend (which is on the log scale)
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_trend(mod, series = 1, data_test = fake_data$data_test)
```

Plot estimated contributions to forecast uncertainty
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_uncertainty(mod, series = 1, data_test = fake_data$data_test)
```

Benchmarking against simplistic "null" models is a very important part of evaluating a proposed forecast model. After all, if our complex dynamic model can't generate better predictions then a random walk or mean forecast, is it really telling us anything new about the data-generating process? Here we examine the model comparison utilities in `mvgam`. First we fit a mis-specified model by smoothing on a white noise covariate rather than on the seasonal variable. Because the white noise covariate is not informative and we are using a random walk for the trend process, this model essentially becomes a Poisson observation model over a dynamic random walk process.
```{r}
fake_data$data_train$fake_cov <- rnorm(NROW(fake_data$data_train))
fake_data$data_test$fake_cov <- rnorm(NROW(fake_data$data_test))
mod2 <- mvjagam(data_train = fake_data$data_train,
               data_test = fake_data$data_test,
               formula = y ~ s(fake_cov, k = 3),
               family = 'poisson',
               trend_model = 'RW',
               n.burnin = 1000,
               n.iter = 1000,
               thin = 1,
               auto_update = F)
```

Look at the model's summary and key plots
```{r}
summary_mvgam(mod2)
```

```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_smooth(mod2, 1, 'fake_cov')
```

```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_fc(mod2, 1)
```

Now we compare the models using rolling probabilistic forecast evaluation. This function sets up a sequence of evaluation timepoints along a rolling window within the training data to evaluate 'out-of-sample' forecasts. The trends are rolled forward a total of `fc_horizon` timesteps according to their estimated state space dynamics to generate an 'out-of-sample' forecast that is evaluated against the true observations in the `horizon` window. We are therefore simulating a situation where the model's parameters had already been estimated but we have only observed data up to the evaluation timepoint and would like to generate forecasts that consider the possible future paths for the latent trends and the true observed values for any other covariates in the `horizon` window. Evaluation involves calculating the Discrete Rank Probability Score and a binary indicator for whether or not the true value lies within the forecast's 90% prediction interval. For this test we compare the two models on the exact same sequence of `30` evaluation points using `horizon = 6`
```{r}
compare_mvgams(mod, mod2, fc_horizon = 6,
               n_evaluations = 30, n_cores = 3)
```

The series of plots generated by `compare_mvgams` clearly show that the first model generates better predictions. In each plot, DRPS for the out of sample `horizon` is lower for the first model than for the second model. This kind of evaluation is often more appropriate for forecast models than complexity-penalising fit metrics such as [`AIC` or `BIC`](https://www.sciencedirect.com/science/article/pii/S0169207020301096). Obviously the autocorrelation in the data indicates that we probably need the dynamic trend model, but we can also explore how our inferences might change if we ignored this autocorrelation process. Here we fit our original model again but set `trend_model = "none"` and use a smooth term for `year` to try and capture the long-term trend (using `B` splines following the excellent example by Gavin Simpson about [extrapolating with smooth terms](https://fromthebottomoftheheap.net/2020/06/03/extrapolating-with-gams/). This is similar to what we might do when fitting a model in `mgcv`, except here we have also an explicit model for the residual component
```{r}
mod_notrend <- mvjagam(data_train = fake_data$data_train,
               data_test = fake_data$data_test,
               formula = y ~ s(season, bs = c('cc'), k = 12) +
                 s(year, bs = "bs", m = c(3, 2, 1, 0)),
               knots = list(season = c(0.5, 12.5),
                            year = seq(min(fake_data$data_train$year),
                                       max(fake_data$data_test$year),
                                       length.out = 4)),
               family = 'nb',
               trend_model = 'None',
               n.burnin = 5000,
               auto_update = F)
```

Have a look at the forecast. The yearly trend is being extrapolated into the future, which controls most of the shape and uncertainty in the forecast
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_fc(mod_notrend, 1, data_test = fake_data$data_test)
```

The "trend" model in this case is just the estimated static residual process
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_trend(mod_notrend, 1)
```

Here is the smooth for the `year`, which highlights how the B spline is starting to extrapolate
```{r, fig.width = 5, fig.height = 4, fig.align='center'}
plot_mvgam_smooth(mod_notrend, 1, 'year')
```

And here are the residual diagnostics, which show that even with this yearly smooth we still have leftover autocorrelation in the residuals
```{r, fig.width = 7, fig.height = 7, fig.align='center'}
plot_mvgam_resids(mod_notrend, 1)
```

Comparing forecasts of these models using the rolling window approach is actually not recommended, as the second model has already seen all the possible in-sample values of `year` and so should be able to predict incredibly well by interpolating through the range of the fitted smooth. By contrast, the dynamic component in the first model produces true forecasts when running the rolling window approach. Nevertheless, when we compare the two models as we did above for the random walk model, we still find that our dynamic GAM produces better probabilistic forecasts
```{r}
compare_mvgams(mod, mod_notrend, fc_horizon = 6,
               n_evaluations = 30, n_cores = 3)
```

A better way of evaluating these models would be to compare their forecasts for the true out of sample period (using observations represented in `fake_data$data_test`. But we can see from comparing their forecast plots that the dynamic model clearly performs better. Now we proceed to exploring how forecast distributions from an `mvgam` object can be automatically updated in light of new incoming observations. This works by generating a set of "particles" that each captures a unique proposal about the current state of the system (in this case, the latent trend component). The next observation in `data_assim` is assimilated and particles are weighted by how well their proposal (prior to seeing the new data) matched the new observations. For univariate models such as the ones we've fitted so far, this weight is represented by the proposal's Negative Binomial log-likelihood. For multivariate models, a multivariate composite likelihood is used for weights. Once weights are calculated, we use importance sampling to update the model's forecast distribution for the remaining forecast horizon. Begin by initiating a set of `10000` particles by assimilating the next observation in `data_test` and storing the particles in the default location (in a directory called `particles` within the working directory)
```{r}
pfilter_mvgam_init(object = mod, n_particles = 10000, n_cores = 2,
                   data_assim = fake_data$data_test)
```

Now we are ready to run the particle filter. This function will assimilate the next six out of sample observations in `data_test` and update the forecast after each assimilation step. This works in an iterative fashion by calculating each particle's weight, then using a kernel smoothing algorithm to "pull" low weight particles toward the high-likelihood space before assimilating the next observation. The strength of the kernel smoother is controlled by `kernel_lambda`, which in our experience works well when left to the default of `1`. If the Effective Sample Size of particles drops too low, suggesting we are putting most of our belief in a very small set of particles, an automatic resampling step is triggered to increase particle diversity and reduce the chance that our forecast intervals become too narrow and incapable of adapting to changing conditions
```{r}
pfilter_mvgam_online(data_assim = fake_data$data_test[1:7,], n_cores = 2,
                     kernel_lambda = 1)
```

Once assimilation is complete, generate the updated forecast from the particles using the covariate information in remaining `data_test` observations. This function is designed to hopefully make it simpler to assimilate observations, as all that needs to be provided once the particles are initiated as a dataframe of test data in exactly the same format as the data that were used to train the initial model. If no new observations are found (observations are arranged by `year` and then by `season` so the consistent indexing of these two variables is very important!) then the function returns a `NULL` and the particles remain where they are in state space.
```{r}
fc <- pfilter_mvgam_fc(file_path = 'pfilter', n_cores = 2,
                       data_test = fake_data$data_test,
                       ylim = c(min(fake_data$data_train$y),
                                max(fake_data$data_test$y)*1.25))
```

Compare the updated forecast to the original forecast to see how it has changed in light of the most recent observations
```{r, fig.width = 7.5, fig.height = 4.5, fig.align='center'}
par(mfrow=c(1,2))
plot_mvgam_fc(mod, series = 1, data_test = fake_data$data_test,
              ylim = c(min(fake_data$data_train$y), 
                       max(fake_data$data_test$y)*1.25))
fc$Air()
points(c(fake_data$data_train$y,
         fake_data$data_test$y), pch = 16)
```

Here it is apparent that the distribution has shifted slightly in light of the `6` observations that have been assimilated, and that our confidence in the remaining forecast horizon has improved (tighter uncertainty intervals). This is an advantageous way of allowing a model to slowly adapt to new conditions while breaking free of restrictive assumptions about residual distributions [see some of the many particle filtering lectures by Nathaniel Osgood for more details](https://www.youtube.com/user/NathanielOsgood). Remove the particles from their stored directory when finished
```{r}
unlink('pfilter', recursive = T)
```

