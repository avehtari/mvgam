---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  dev = "png",
  dpi = 160,
  fig.asp = 0.8,
  fig.width = 6,
  out.width = "60%",
  fig.align = "center"
)
```

*mvgam*
================

The goal of `mvgam` is to use a Bayesian framework to estimate parameters of Generalized Additive Models (DGAMs) for time series with dynamic trend components. The motivation for the package and some of its primary objectives are described in detail by [Clark & Wells 2022](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13974){target="_blank"} (published in *Methods in Ecology and Evolution*), with additional inspiration on the use of Bayesian probabilistic modelling to quantify uncertainty and advise principled decision making coming from [Michael Betancourt](https://betanalpha.github.io/writing/){target="_blank"}, [Michael Dietze](https://www.bu.edu/earth/profiles/michael-dietze/){target="_blank"} and [Emily Fox](https://emilybfox.su.domains/){target="_blank"}, among many others. 

## Resources
A number of case studies have been compiled to highlight how DGAMs can be estimated using MCMC sampling. These are hosted currently on `RPubs` at the following links:
  
* [mvgam case study 1: model comparison and data assimilation](https://rpubs.com/NickClark47/mvgam){target="_blank"}
* [mvgam case study 2: multivariate models](https://rpubs.com/NickClark47/mvgam2){target="_blank"}
* [mvgam case study 3: distributed lag models](https://rpubs.com/NickClark47/mvgam3){target="_blank"}
  
The package can also be used to generate all necessary data structures, initial value functions and modelling code necessary to fit DGAMs using `Stan` or `JAGS`. This can be helpful if users wish to make changes to the model to better suit their own bespoke research / analysis goals. The following resources can be helpful to troubleshoot:
  
* [Stan Discourse](https://discourse.mc-stan.org/){target="_blank"}
* [JAGS Discourse](https://sourceforge.net/projects/mcmc-jags/){target="_blank"}
  
## Installation
Install the development version from `GitHub` using:
`devtools::install_github("nicholasjclark/mvgam")`. Note that to actually condition models with MCMC sampling, either the `JAGS` software must be installed (along with the `R` packages `rjags` and `runjags`) or the `Stan` software must be installed (along with either `rstan` and/or `cmdstanr`). Only `rstan` is listed as a dependency of `mvgam` to ensure that installation is less difficult. If users wish to fit the models using `mvgam`, please refer to installation links for `JAGS` [here](https://sourceforge.net/projects/mcmc-jags/files/){target="_blank"}, for `Stan` with `rstan` [here](https://mc-stan.org/users/interfaces/rstan){target="_blank"}, or for `Stan` with `cmdstandr` [here](https://mc-stan.org/cmdstanr/){target="_blank"}. You will need a fairly recent version of `Stan` to ensure all the model syntax is recognized. If you see warnings such as `variable "array" does not exist`, this is usually a sign that you need to update your version of `Stan`. We highly recommend you use `Cmdstan` through the `cmdstanr` interface as the backend. This is because `Cmdstan` is easier to install, is more up to date with new features, and uses less memory than `Rstan`. See [this documentation from the `Cmdstan` team for more information](http://mc-stan.org/cmdstanr/articles/cmdstanr.html#comparison-with-rstan){target="_blank"}. 

## Citing mvgam and related software
When using open source software (or software in general), please make sure to appropriately acknowledge the hard work that developers and maintainers put into making these packages available. Citations are currently the best way to formally acknowledge this work, so we highly encourage you to cite any packages that you rely on for your research.

When using `mvgam`, please cite the following publication:

- Clark, N.J. and Wells, K. (2022). Dynamic Generalized Additive Models (DGAMs) for forecasting discrete ecological time series. *Methods in Ecology and Evolution*. DOI: https://doi.org/10.1111/2041-210X.13974

As `mvgam` acts as an interface to `Stan` and `JAGS`, please additionally cite whichever software you use for parameter estimation:

- Carpenter B., Gelman A., Hoffman M. D., Lee D., Goodrich B., Betancourt M.,
  Brubaker M., Guo J., Li P., and Riddell A. (2017). Stan: A probabilistic
  programming language. *Journal of Statistical Software*. 76(1).
  10.18637/jss.v076.i01
- Plummer, M. (2013). JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling.          *Proceedings of the 3rd International Workshop on Distributed Statistical Computing*. 124(125.10).

`mvgam` relies on several other `R` packages and, of course, on `R` itself. To
find out how to cite R and its packages, use the `citation` function. There are
some features of `mvgam` which specifically rely on certain packages. The most important of these is the generation of data necessary to estimate smoothing splines, which entirely rely on `mgcv`. The `rstan` and `cmdstanr` packages together with `Rcpp` makes `Stan` conveniently accessible in `R`, while the `rjags` and `runjags` packages together with the `coda` package make `JAGS` accessible in `R`. If you use some of these features, please also consider citing the related packages.

## Dynamic latent temporal processes
`mvgam` is designed to propagate unobserved temporal processes to capture autocorrelation in the observed time series. This works in a state-space format, with the temporal *trend* evolving independently of the observation process. Available trend models are: 
  
* `RW` Random Walk
* `AR1` Autoregressive model with AR coefficient for lag 1
* `AR2` Autoregressive model with AR coefficients for lags 1 and 2
* `AR3` Autoregressive model with AR coefficients for lags 1, 2 and 3 
* `VAR1` Vector Autoregressive model with VAR coefficients for lag 1; contemporaneously uncorrelated errors 
* `VAR1cor` Vector Autoregressive model with VAR coefficients for lag 1; contemporaneously correlated (multivariate) errors 
* `GP` Squared exponential Gaussian Process 
* `None` No latent trend is fitted
  
When using `Stan` as the back-end, all of these trend models (apart from `VAR` models) can be estimated using a set of dimension-reduced dynamic factors. Please see [mvgam case study 2: multivariate models](https://rpubs.com/NickClark47/mvgam2){target="_blank"} for more information

## A brief introduction to the package
We can explore the modelâ€™s primary functions using a test dataset that
is available with all `R` installations. We introduce Dynamic Generalized Additive Models and some of the key utility functions provided in `mvgam`. First, load the `lynx` data and plot the series as well as its estimated autocorrelation function
```{r}
library(mvgam)
data(lynx)
lynx_full = data.frame(year = 1821:1934, 
                       population = as.numeric(lynx))
plot(lynx_full$population, type = 'l', ylab = 'Lynx trappings',
     xlab = 'Time', bty = 'l', lwd = 2)
box(bty = 'l', lwd  = 2)
acf(lynx_full$population, main = '', bty = 'l', lwd = 2,
    ci.col = 'darkred')
box(bty = 'l', lwd  = 2)
```

Along with serial autocorrelation, there is a clear ~19-year cyclic pattern to the data. Create a `season` term that can be used to model this effect and give a better representation of the data generating process than we would likely get with a linear model
```{r}
plot(stl(ts(lynx_full$population, frequency = 19), s.window = 'periodic'),
     lwd = 2, col.range = 'darkred')
lynx_full$season <- (lynx_full$year %%19) + 1
```

For `mvgam` models, we need an indicator of the series name as a `factor` variable (if the column `series` is missing, this will be added automatically by assuming that all observations are from a single time series). Finally, a `time` column is needed to index time
```{r}
lynx_full$time <- 1:NROW(lynx_full)
lynx_full$series <- factor('series1')
```

Split the data into training (first 50 years) and testing (next 10 years of data) to evaluate multi-step ahead forecasts
```{r}
lynx_train = lynx_full[1:50, ]
lynx_test = lynx_full[51:60, ]
```

Inspect the series in a bit more detail using `mvgam`'s plotting utility
```{r, message=FALSE, warning=FALSE, fig.width=6.5, fig.height=6.5, dpi=160}
plot_mvgam_series(data = lynx_train, y = 'population')
```

Now we will formulate an `mvgam` model; this model fits a GAM in which a cyclic smooth function for `season` is estimated jointly with a full time series model for the temporal process (in this case an `AR3` process), rather than relying on smoothing splines that do not incorporate a concept of the future. We assume the outcome follows a Poisson distribution. But before conditioning the model on observed data, a check of prior smooth function realisations is useful to ensure we are allowing enough flexibility to capture the types of functional behaviours we think are reasonable without allowing outrageous behaviours. First we follow conventional recommendations to set `k` for the smooth term to be large, which would allow maximum flexibility in functional behaviours
```{r, eval=FALSE}
lynx_mvgam_prior <- mvgam(data = lynx_train,
               formula = population ~ s(season, bs = 'cc', k = 19),
               knots = list(season = c(0.5, 19.5)),
               family = 'poisson',
               trend_model = 'AR3',
               chains = 4,
               prior_simulation = TRUE)
```

```{r, include=FALSE}
lynx_mvgam_prior <- mvgam(data = lynx_train,
               formula = population ~ s(season, bs = 'cc', k = 19),
               knots = list(season = c(0.5, 19.5)),
               family = 'poisson',
               trend_model = 'AR3',
               chains = 4,
               prior_simulation = TRUE)
```


Plot empirical quantiles of the prior seasonal smooth function
```{r}
plot(lynx_mvgam_prior, type = 'smooths')
```

Plot a set of realisations from the prior seasonal smooth function
```{r}
plot(lynx_mvgam_prior, type = 'smooths', realisations = TRUE,
     n_realisations = 20)
```

These functions are showing the marginal contribution of the seasonal smooth function to the linear predictor (on the log scale), and they are clearly allowed to move into ridiculous spaces that we should give very little prior plausibility to:
```{r}
exp(-15)
exp(15)
```

Setting `k` to a smaller value results in less flexibility. This is because number of basis functions that contribute to functional behaviour is reduced
```{r, include = FALSE}
lynx_mvgam_prior <- mvgam(data = lynx_train,
               formula = population ~ s(season, bs = 'cc', k = 12),
               knots = list(season = c(0.5, 19.5)),
               family = 'poisson',
               trend_model = 'AR3',
               chains = 4,
               prior_simulation = TRUE)
```

```{r, eval = FALSE}
lynx_mvgam_prior <- mvgam(data = lynx_train,
               formula = population ~ s(season, bs = 'cc', k = 12),
               knots = list(season = c(0.5, 19.5)),
               family = 'poisson',
               trend_model = 'AR3',
               chains = 4,
               prior_simulation = TRUE)
```

The resulting prior looks more reasonable given the range of the observations, and there is clearly enough flexibility to support a wide range of functional shapes.
```{r}
plot(lynx_mvgam_prior, type = 'smooths')
```

```{r}
plot(lynx_mvgam_prior, type = 'smooths', realisations = TRUE,
     n_realisations = 20)
```

In practice, imparting domain knowledge into prior specifications for penalised smooth functions is challenging, as these behaviours are often the cumulative result of multiple penalty matrices that all have their own separate smoothing parameters. Changing the prior on the smoothing parameters is another option (`mvgam` uses a half-normal prior by default, which regularises functions more than the default approach used in `mgcv::jagam`). But without running through prior visualisations (and other prior pushforward checks) it will be more difficult to reason about how to set `k` to respect domain knowledge. In general it is highly recommended that users view `mvgam` and related interfaces such as `brms` as tools for building scaffold models that can then be modified to suit the bespoke needs of each particular analysis. 

Users can also check what the default prior distributions are for given model formulations, which can be helpful to understand how the model can be modified but also to see any restrictions on what can be changed within the `mvgam` framework.

```{r}
test_priors <- get_mvgam_priors(population ~ s(season, bs = 'cc', k = 12),
                                family = 'poisson',
                                data = lynx_train,
                                trend_model = 'AR3',
                                use_stan = TRUE)
test_priors
```

Any of the above priors can be changed by modifying the `prior` column and supplying the resulting `data.frame` to the `priors` argument in `mvgam()`. But for now, we will proceed with the defaults by conditioning the model on observed data in `Stan` using MCMC sampling with the `Cmdstan` interface (installation links for `rstan` and `cmdstanr` are found [here](https://mc-stan.org/users/interfaces/rstan){target="_blank"} and [here](https://mc-stan.org/cmdstanr/articles/cmdstanr.html){target="_blank"}).
```{r, include=FALSE}
lynx_mvgam <- mvgam(data = lynx_train,
               newdata = lynx_test,
               formula = population ~ s(season, bs = 'cc', k = 12),
               knots = list(season = c(0.5, 19.5)),
               family = 'poisson',
               trend_model = 'AR3',
               use_stan = TRUE)
```

```{r, eval=FALSE}
lynx_mvgam <- mvgam(data = lynx_train,
               newdata = lynx_test,
               formula = population ~ s(season, bs = 'cc', k = 12),
               knots = list(season = c(0.5, 19.5)),
               family = 'poisson',
               trend_model = 'AR3',
               use_stan = TRUE)
```

Inspect the resulting model file, which is written in the `Stan` probabilistic programming language
```{r}
code(lynx_mvgam)
```

Perform a series of posterior retrodictive checks to see if the model is able to simulate data for the training period that looks realistic and unbiased. First, examine histograms for posterior retrodictions (`yhat`) and compare to the histogram of the observations (`y`)
```{r}
ppc(lynx_mvgam, series = 1, type = 'hist')
```

Next examine simulated empirical Cumulative Distribution Functions (CDF) for posterior retrodictions (`yhat`) and compare to the CDF of the observations (`y`)
```{r}
ppc(lynx_mvgam, series = 1, type = 'cdf')
```

Rootograms are becoming [popular graphical tools for checking a discrete model's ability to capture dispersion properties of the response variable](https://arxiv.org/pdf/1605.01311.pdf){target="_blank"}. Posterior predictive hanging rootograms can be displayed using the `ppc()` function in `mvgam`. In the plot below, we bin the unique observed values into `25` bins to prevent overplotting and help with interpretation. This plot compares the frequencies of observed vs predicted values for each bin, which can help to identify aspects of poor model fit. For example, if the gray bars (representing observed frequencies) tend to stretch below zero, this suggests the model's simulations predict the values in that particular bin less frequently than they are observed in the data. A well-fitting model that can generate realistic simulated data will provide a rootogram in which the lower boundaries of the grey bars are generally near zero
```{r}
ppc(lynx_mvgam, series = 1, type = 'rootogram', n_bins = 25)
```

Finally look for any biases in predictions by examining a Probability Integral Transform (PIT) histogram. If our predictions are not biased one way or another (i.e. not consistently under- or over-predicting), this histogram should look roughly uniform
```{r}
ppc(lynx_mvgam, series = 1, type = 'pit')
```

All of these plots indicate the model is well calibrated against the training data, with no apparent pathological behaviors exhibited. Have a look at this model's summary to see what is being estimated. Note that no pathological behaviours have been detected and we achieve good effective sample sizes / mixing for all parameters
```{r}
summary(lynx_mvgam)
```

As with any `MCMC` based software, we can inspect traceplots. Here for the `GAM` component smoothing parameters, using `mvgam`'s reliance on the excellent `bayesplot` library:
```{r}
mcmc_plot(lynx_mvgam, variable = 'rho', regex = TRUE, type = 'trace')
```

and for the latent trend component parameters
```{r}
mcmc_plot(lynx_mvgam, variable = 'trend_params', regex = TRUE, type = 'trace')
```

Inspect the model's estimated smooth for the 19-year cyclic pattern, which is shown as a ribbon plot of posterior empirical quantiles. We can also overlay posterior quantiles of partial residuals (shown as ribbon rectangles in red), which represent the leftover variation that the model expects would remain if this smooth term was dropped but all other parameters remained unchanged. Note that these are on a different scale to those from `mgcv::plot.gam` as these are randomised quantile residuals that are essentially standard normal in distribution. But either way, a strong pattern in the partial residuals suggests there would be strong patterns left unexplained in the model *if* we were to drop this term, giving us further confidence that this function is important in the model
```{r, fig.width=5, fig.height=4, fig.align='center', dpi=150}
plot(lynx_mvgam, type = 'smooths', residuals = TRUE)
```

It is often also useful to compare prior to posterior function realisations to understand how informative the observed data have been for learning these functional shapes
```{r}
layout(matrix(1:2, nrow = 2))
plot(lynx_mvgam_prior, type = 'smooths', realisations = TRUE,
     n_realisations = 30)
plot(lynx_mvgam, type = 'smooths', realisations = TRUE,
     n_realisations = 30)
layout(1)
```

First derivatives of smooth functions can be plotted to inspect how the slope of the function changes across its length. To plot these we use the more flexible `plot_mvgam_smooth()` function
```{r}
plot_mvgam_smooth(lynx_mvgam, series = 1, 
                  smooth = 'season', 
                  derivatives = TRUE)
```

As for many types of regression models, it is often more useful and intuitive to plot model effects on the scale of the outcome. `mvgam` has support for the wonderful `marginaleffects` package, allowing a wide variety of posterior contrasts, averages, conditional and marginal predictions to be seamlessly calculated and plotted. Below is the conditional effect of season plotted on the outcome scale, for example:
```{r}
require(ggplot2)
plot_predictions(lynx_mvgam, condition = 'season', points = 0.5) +
  theme_classic()
```

We can also view the `mvgam`'s posterior retrodictions and predictions for the entire series (testing and training)
```{r}
plot(lynx_mvgam, type = 'forecast', newdata = lynx_test)
```

And the estimated latent trend component, again using the more flexible `plot_mvgam_...()` option to show first derivatives of the estimated trend
```{r}
plot_mvgam_trend(lynx_mvgam, newdata = lynx_test, derivatives = TRUE)
```

We can also re-do the posterior predictive checks, but this time focusing only on the out of sample period. This will give us better insight into how the model is performing and whether it is able to simulate realistic and unbiased future values
```{r}
ppc(lynx_mvgam, series = 1, type = 'rootogram', newdata = lynx_test)
```

```{r}
ppc(lynx_mvgam, series = 1, type = 'cdf', newdata = lynx_test)
```

```{r}
ppc(lynx_mvgam, series = 1, type = 'pit', newdata = lynx_test)
```

A key aspect of ecological forecasting is to understand [how different components of a model contribute to forecast uncertainty](https://esajournals.onlinelibrary.wiley.com/doi/full/10.1002/eap.1589){target="_blank"}. We can estimate relative contributions to forecast uncertainty for the GAM component and the latent trend component using `mvgam`
```{r}
plot_mvgam_uncertainty(lynx_mvgam, newdata = lynx_test, legend_position = 'none')
text(1, 0.2, cex = 1.5, label="GAM component", 
     pos = 4, col="white", family = 'serif')
text(1, 0.8, cex = 1.5, label="Trend component", 
     pos = 4, col="#7C0000", family = 'serif')
```

Both components contribute to forecast uncertainty, suggesting we would still need some more work to learn about factors driving the dynamics of the system. But we will leave the model as-is for this example. Diagnostics of the model can also be performed using `mvgam`. Have a look at the model's residuals, which are posterior empirical quantiles of Dunn-Smyth randomised quantile residuals so should follow approximate normality. We are primarily looking for a lack of autocorrelation, which would suggest our AR3 model is appropriate for the latent trend
```{r, fig.width=6.5, fig.height=6.5, dpi=160}
plot(lynx_mvgam, type = 'residuals')
```

## Comparing models based on forecasts
Another useful utility of `mvgam` is the ability to use approximate rolling window forecasts to evaluate competing models that may represent different hypotheses about the series dynamics. Here we will fit a poorly specified model to showcase how these evaluations works. In this model, we ignore the cyclic pattern of seasonality. We also use a random walk process for the trend
```{r, include=FALSE}
lynx_mvgam_poor <- mvgam(data = lynx_train,
               newdata = lynx_test,
               formula = population ~ 1,
               family = 'poisson',
               trend_model = 'RW',
               use_stan = TRUE,
               chains = 4)
```

```{r, eval=FALSE}
lynx_mvgam_poor <- mvgam(data = lynx_train,
               newdata = lynx_test,
               formula = population ~ 1,
               family = 'poisson',
               trend_model = 'RW',
               use_stan = TRUE,
               chains = 4)
```

The first approximator targets each model's ability to simulate temporal dynamics using the single model that has been fit. We choose a set of timepoints within the training data to forecast from, allowing us to simulate a situation where the model's parameters had already been estimated but we have only observed data up to the evaluation timepoint and would like to generate forecasts from the latent trends. Here we simulate scenarios where we forecast ahead for the next 10 years. The `compare_mvgams` function automates this process by rolling along a set of timepoints for each model, ensuring a more in-depth evaluation of each competing model at the same set of timepoints. 
```{r}
compare_mvgams(lynx_mvgam, lynx_mvgam_poor, fc_horizon = 10)
```

Summary statistics of the two models' out of sample Discrete Rank Probability Score (DRPS) indicate that the well-specified model performs markedly better (lower DRPS) across most out of sample horizons.

The second approximator uses more conventional leave-future-out comparisons. Time series models are often evaluated using an expanding window training technique, where the model is initially trained on some subset of data from `t = 1` to `t = n_train`, and then is used to produce forecasts for the next `fc_horizon` time steps `t = n_train + fc_horizon`. In the next iteration, the size of training data is expanded by a single time point and the process repeated. This is obviously computationally challenging for Bayesian time series models, as the number of refits can be very large. `mvgam` uses an approximation based on importance sampling. Briefly, we refit the model using the first `min_t` observations to perform a single exact `fc_horizon`-ahead forecast step. This forecast is evaluated against the `min_t + fc_horizon` out of sample observations using the Expected Log Predictive Density (ELPD). Next, we approximate each successive round of expanding window forecasts by moving forward one step at a time `for i in 1:N_evaluations` and re-weighting draws from the model's posterior predictive distribution using Pareto Smoothed Importance Sampling (PSIS). In each iteration `i`, PSIS weights are obtained for all observations that would have been included in the model if we had re-fit. If these importance ratios are stable, we consider the approximation adequate and use the re-weighted posterior's forecast for evaluating the next holdout set of testing observations (`(min_t + i + 1):(min_t + i + fc_horizon)`). This is similar to the process of particle filtering to update forecasts in light of new data by re-weighting the posterior draws using importance weights. But at some point the importance ratio variability will become too large and importance sampling will be unreliable. This is indicated by the estimated shape parameter `k` of the generalized Pareto distribution crossing a certain threshold `pareto_k_threshold`. Only then do we refit the model using all of the observations up to the time of the failure. We then restart the process and iterate forward until the next refit is triggered. The process is computationally much more efficient, as only a fraction of the evaluations typically requires refits (the algorithm isdescribed in detail by BÃ¼rkner et al. 2020).

Paul-Christian BÃ¼rkner, Jonah Gabry & Aki Vehtari (2020). Approximate leave-future-out cross-validation for Bayesian time series models. Journal of Statistical Computation and Simulation. 90:14, 2499-2523.

For this example, we simulate a single count-valued time series of length `T = 60`, using a latent `AR1` trend and a cyclic seasonal pattern. Two models are fit as before, the first is a complex but more correct model, while the second is simpler and mis-specified. We then run approximate leave-future-out cross-validation, setting `min_t = 36` so that the first refit uses all observations from `t = 1` to `t = 36`. This is done for both models so that we can compare approximate ELPD values when forecasting two time steps ahead (`fc_horizon = 2`)
```{r, eval=FALSE}
set.seed(12345)
simdat <- sim_mvgam(T = 60, prop_train = 1, n_series = 1,
                    mu = 2,
                    trend_model = 'AR1', trend_rel = 0.35)
good <- mvgam(y ~ s(season, bs = 'cc', k = 8),
              trend_model = 'AR1',
              family = poisson(),
              data = simdat$data_train)
poor <- update(good, formula = y ~ 1,
               trend_model = 'RW')
lfo_good <- lfo_cv(good, min_t = 36,
                   fc_horizon = 2)
lfo_poor <- lfo_cv(poor, min_t = 36,
                   fc_horizon = 2)
```

```{r, include=FALSE}
set.seed(12345)
simdat <- sim_mvgam(T = 60, prop_train = 1, n_series = 1,
                    mu = 2,
                    trend_model = 'AR1', trend_rel = 0.35)
good <- mvgam(y ~ s(season, bs = 'cc', k = 8),
              trend_model = 'AR1',
              family = poisson(),
              data = simdat$data_train)
poor <- update(good, formula = y ~ 1,
               trend_model = 'RW')
lfo_good <- lfo_cv(good, min_t = 36,
                   fc_horizon = 2)
lfo_poor <- lfo_cv(poor, min_t = 36,
                   fc_horizon = 2)
```

The `S3` plotting function for these `lfo_cv` objects will show the Pareto-k values and ELPD values over the evaluation time points. For the Pareto-k plot, a dashed red line indicates the specified threshold chosen for triggering model refits. For the ELPD plot, a dashed red line indicates the bottom 10% quantile of ELPD values. Points below this threshold may represent outliers that were more difficult to forecast
```{r, fig.width=6.5, fig.height=7.5, dpi=160}
par(mar = c(4,4, 1, 1))
plot(lfo_good)
```

```{r, fig.width=6.5, fig.height=7.5, dpi=160}
par(mar = c(4,4, 1, 1))
plot(lfo_poor)
```

The model with the better ELPD values (higher values are better in this case) should be preferred. First we can calculate the proportion of forecast time points in which the first model gives better forecasts than the poor model
```{r}
length(which((lfo_good$elpds - lfo_poor$elpds) > 0)) /
  length(lfo_good$elpds)
```

Total ELPDs per model are also a useful overall indicator of performance
```{r}
lfo_good$sum_ELPD
lfo_poor$sum_ELPD
```

As before, these metrics all favour the more complex model over the simpler "poor" model. This gives us confidence that the more complex model will perform better in future forecasting exercises. The first approximator is faster as it does not require refits, but caution is needed as the state of the latent trend at the evaluation time point has already been informed by both the past and future observations. The second approximator, using PSIS for approximate leave-future-out, should be preferred when computationally accessible.

## Extended observation families
`mvgam` was originally designed to analyse and forecast non-negative integer-valued data (counts). These data are traditionally challenging to analyse with existing time-series analysis packages. But further development of `mvgam` has resulted in support for a growing number of observation families that extend to other types of data. Currently, the package can handle data for the following families:  
  
* `gaussian()` for real-valued data 
* `student_t()` for heavy-tailed real-valued data
* `lognormal()` for non-negative real-valued data
* `Gamma()` for non-negative real-valued data
* `betar()` for proportional data on `(0,1)`
* `poisson()` for count data
* `nb()` for overdispersed count data
* `tweedie()` for overdispersed count data
  
Note that only `poisson()`, `nb()`, and `tweedie()` are available if using `JAGS`. All families, apart from `tweedie()`, are supported if using `Stan`. See `??mvgam_families` for more information. Below is a simple example for simulating and modelling proportional data with `Beta` observations over a set of seasonal series with independent Gaussian Process dynamic trends:
```{r beta_sim, message=FALSE, warning=FALSE, fig.width=6.5, fig.height=6.5, dpi=160}
set.seed(100)
data <- sim_mvgam(family = betar(),
                 T = 80,
                 trend_model = 'GP',
                 trend_rel = 0.5, 
                 seasonality = 'shared')
plot_mvgam_series(data = data$data_train, series = 'all')
```

```{r, include=FALSE}
mod <- mvgam(y ~ s(season, bs = 'cc', k = 7) +
               s(season, by = series, m = 1, k = 5),
             trend_model = 'GP',
             data = data$data_train,
             newdata = data$data_test,
             family = betar())
```

```{r, eval=FALSE}
mod <- mvgam(y ~ s(season, bs = 'cc', k = 7) +
               s(season, by = series, m = 1, k = 5),
             trend_model = 'GP',
             data = data$data_train,
             newdata = data$data_test,
             family = betar())
```

Inspect the summary to see that the posterior now also contains estimates for the `Beta` precision parameters $\phi$. Note that we can also suppress a summary of the $\beta$ coefficients as an option, which is useful when there are many spline coefficients to report:
```{r}
summary(mod, include_betas = FALSE)
```

Plot the hindcast and forecast distributions for each series
```{r eval=FALSE}
layout(matrix(1:4, nrow = 2, byrow = TRUE))
for(i in 1:3){
  plot(mod, type = 'forecast', series = i)
}
```

```{r beta_fc, echo=FALSE}
layout(matrix(1:4, nrow = 2, byrow = TRUE))
par(mar=c(3, 3, 2, 2),
    oma = c(2, 2, 0, 0),
    mgp = c(2, 0.5, 0))
for(i in 1:3){
  plot(mod, type = 'forecast', series = i)
}
```

There are many more extended uses for `mvgam` models, including the ability to fit dynamic coefficient models, dynamic factor and vector autoregressive processes for analysing and forecasting sets of multivariate time series and fitting hierarchical GAMs. See the package documentation for more details.

## License
This project is licensed under an `MIT` open source license
